{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nIMAGE_DIR = \"/kaggle/input/clothes-tryon/clothes_tryon_dataset/train/cloth\"\nMASK_DIR = \"/kaggle/input/clothes-tryon/clothes_tryon_dataset/train/cloth-mask\"\nMODEL_SAVE_PATH = \"unet_clothing_segmentation.pth\"\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nLEARNING_RATE = 1e-4\nBATCH_SIZE = 16 \nEPOCHS = 10 \nIMAGE_HEIGHT = 256\nIMAGE_WIDTH = 192\n\nprint(f\"Using device: {DEVICE}\")\nprint(f\"Image directory: {IMAGE_DIR}\")\nprint(f\"Mask directory: {MASK_DIR}\")\n\ntransform = transforms.Compose([\n    transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nmask_transform = transforms.Compose([\n    transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n    transforms.ToTensor(),\n])\n\nclass ClothingDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, img_transform, mask_transform):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.img_transform = img_transform\n        self.mask_transform = mask_transform\n        # Ensure we only load files that exist in both directories\n        self.image_files = sorted(os.listdir(image_dir))\n        self.mask_files = sorted(os.listdir(mask_dir))\n        self.images = [f for f in self.image_files if f in self.mask_files]\n        print(f\"Found {len(self.images)} matching image-mask pairs.\")\n\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_name = self.images[idx]\n        img_path = os.path.join(self.image_dir, img_name)\n        mask_path = os.path.join(self.mask_dir, img_name)\n        \n        image = Image.open(img_path).convert(\"RGB\")\n        mask = Image.open(mask_path).convert(\"L\")\n        \n        image = self.img_transform(image)\n        mask = self.mask_transform(mask)\n        \n        return image, mask\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\nclass Down(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\nclass Up(nn.Module):\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # Pad x1 to the size of x2\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)\n\nclass UNet(nn.Module):\n    def __init__(self, n_channels=3, n_classes=1, bilinear=True):\n        super(UNet, self).__init__()\n        self.inc = DoubleConv(n_channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        self.down3 = Down(256, 512)\n        factor = 2 if bilinear else 1\n        self.down4 = Down(512, 1024 // factor)\n        self.up1 = Up(1024, 512 // factor, bilinear)\n        self.up2 = Up(512, 256 // factor, bilinear)\n        self.up3 = Up(256, 128 // factor, bilinear)\n        self.up4 = Up(128, 64, bilinear)\n        self.outc = OutConv(64, n_classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return logits\n\n# --- STEP 3: LOSS FUNCTION ---\nclass DiceBCELoss(nn.Module):\n    def __init__(self):\n        super(DiceBCELoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        inputs_sig = torch.sigmoid(inputs)\n        inputs_flat = inputs_sig.view(-1)\n        targets_flat = targets.view(-1)\n        \n        intersection = (inputs_flat * targets_flat).sum()                            \n        dice_loss = 1 - (2.*intersection + smooth)/(inputs_flat.sum() + targets_flat.sum() + smooth)  \n        bce = F.binary_cross_entropy_with_logits(inputs, targets, reduction='mean')\n        \n        return bce + dice_loss\ndef train_one_epoch(loader, model, optimizer, loss_fn, scaler):\n    loop = tqdm(loader, leave=True)\n    running_loss = 0.0\n\n    for batch_idx, (data, targets) in enumerate(loop):\n        data = data.to(device=DEVICE)\n        targets = targets.float().to(device=DEVICE)\n        with torch.cuda.amp.autocast():\n            predictions = model(data)\n            loss = loss_fn(predictions, targets)\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        running_loss += loss.item()\n        loop.set_postfix(loss=loss.item())\n    \n    return running_loss / len(loader)\n\ndef main():\n    model = UNet(n_channels=3, n_classes=1).to(DEVICE)\n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    loss_fn = DiceBCELoss()\n    scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == 'cuda'))\n    \n    full_dataset = ClothingDataset(\n        image_dir=IMAGE_DIR, \n        mask_dir=MASK_DIR,\n        img_transform=transform,\n        mask_transform=mask_transform\n    )\n\n    train_size = int(0.9 * len(full_dataset))\n    val_size = len(full_dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n        if os.path.exists(MODEL_SAVE_PATH):\n        print(f\"Loading pre-trained model from {MODEL_SAVE_PATH}\")\n        model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n    else:\n        print(\"--- Starting Training ---\")\n        for epoch in range(EPOCHS):\n            # Pass the scaler to the training function\n            train_loss = train_one_epoch(train_loader, model, optimizer, loss_fn, scaler)\n            print(f\"Epoch {epoch+1}/{EPOCHS} - Training Loss: {train_loss:.4f}\")\n        \n        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n        print(f\"Model saved to {MODEL_SAVE_PATH}\")\n\n    print(\"--- Visualizing Predictions on Validation Set ---\")\n    model.eval()\n    num_images_to_show = 4\n    fig, axes = plt.subplots(num_images_to_show, 3, figsize=(12, num_images_to_show * 4))\n    \n    with torch.no_grad():\n        for i, (x, y) in enumerate(val_loader):\n            if i >= num_images_to_show:\n                break\n            \n            x = x.to(DEVICE)\n            with torch.cuda.amp.autocast():\n                preds = torch.sigmoid(model(x))\n            preds = (preds > 0.5).float()\n\n            original_img = x[0].cpu().permute(1, 2, 0).numpy()\n            ground_truth = y[0].cpu().squeeze().numpy()\n            prediction = preds[0].cpu().squeeze().numpy()\n            \n            mean = np.array([0.485, 0.456, 0.406])\n            std = np.array([0.229, 0.224, 0.225])\n            original_img = std * original_img + mean\n            original_img = np.clip(original_img, 0, 1)\n\n            axes[i, 0].imshow(original_img)\n            axes[i, 0].set_title(\"Original Image\")\n            axes[i, 0].axis(\"off\")\n\n            axes[i, 1].imshow(ground_truth, cmap='gray')\n            axes[i, 1].set_title(\"Ground Truth Mask\")\n            axes[i, 1].axis(\"off\")\n\n            axes[i, 2].imshow(prediction, cmap='gray')\n            axes[i, 2].set_title(\"Predicted Mask\")\n            axes[i, 2].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}