# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NzWfHaV8VVDsvIpf9dQMQmai5ONriTpf
"""

!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.0.0+cpu.html
!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cpu.html
!pip install -q torch-geometric

from google.colab import drive
drive.mount('/content/drive')  # Mount your Google Drive

zip_path = "/content/drive/MyDrive/Colab Notebooks/archive.zip"

import zipfile
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall("unzipped_drive")  # Extracts contents to folder

!wget https://raw.githubusercontent.com/yisol/IDM-VTON/main/vitonhd_test_tagged.json -P /content/

import os
import json

# Define path to cloth directory and JSON annotation
cloth_dir = "/content/unzipped_drive/clothes_tryon_dataset/test/cloth"
json_path = '/content/vitonhd_test_tagged.json'

# Load JSON
with open(json_path, 'r') as f:
    json_data = json.load(f)["data"]

# Build a lookup dict from JSON: filename -> tags
json_lookup = {item["file_name"]: item for item in json_data}

# Get list of cloth images in directory
cloth_images = sorted(os.listdir(cloth_dir))

# Match and print sample metadata
matched_count = 0
for filename in cloth_images:
    if filename in json_lookup:
        metadata = json_lookup[filename]
        print(f"\n‚úÖ Matched: {filename}")
        print("Category:", metadata["category_name"])
        print("Tags:")
        for tag in metadata["tag_info"]:
            print(f"  - {tag['tag_category']}: {tag['tag_name']}")
        matched_count += 1
    if matched_count >= 5:
        break

if matched_count == 0:
    print("‚ùå No matches found between cloth_dir and JSON keys.")

import os
import json
import matplotlib.pyplot as plt
from PIL import Image

# Paths
cloth_dir = "/content/unzipped_drive/clothes_tryon_dataset/test/cloth"
json_path = "/content/vitonhd_test_tagged.json"

# Load JSON
with open(json_path, 'r') as f:
    json_data = json.load(f)["data"]

# Build a lookup: filename -> metadata
json_lookup = {item["file_name"]: item for item in json_data}

# Get list of images in cloth_dir
cloth_images = sorted(os.listdir(cloth_dir))

# Show up to N matches
N = 5
shown = 0

for filename in cloth_images:
    if filename in json_lookup:
        metadata = json_lookup[filename]
        tags = metadata["tag_info"]

        # Load image
        img_path = os.path.join(cloth_dir, filename)
        image = Image.open(img_path).convert("RGB")

        # Prepare text description
        tag_lines = [f"{tag['tag_category']}: {tag['tag_name']}" for tag in tags]
        tag_text = f"Filename: {filename}\nCategory: {metadata['category_name']}\n\nTags:\n" + "\n".join(tag_lines)

        # Plot
        plt.figure(figsize=(10, 5))
        plt.subplot(1, 2, 1)
        plt.imshow(image)
        plt.axis("off")
        plt.title("Cloth Image")

        plt.subplot(1, 2, 2)
        plt.axis("off")
        plt.text(0, 1, tag_text, va="top", fontsize=10, wrap=True, family='monospace')
        plt.title("Tags")

        plt.tight_layout()
        plt.show()

        shown += 1
        if shown >= N:
            break

if shown == 0:
    print("‚ùå No matches found.")

import os
import json

# Paths
cloth_dir = "/content/unzipped_drive/clothes_tryon_dataset/test/cloth"
json_path = "/content/vitonhd_test_tagged.json"

# Load JSON
with open(json_path, 'r') as f:
    json_data = json.load(f)["data"]

# Get list of images in cloth_dir
cloth_images = sorted(os.listdir(cloth_dir))

# Filter JSON entries whose filenames are in the cloth folder
matched_data = [item for item in json_data if item["file_name"] in cloth_images]

print(f"‚úÖ Total matched items: {len(matched_data)}")

# import json
# import os
# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# from tqdm import tqdm
# import networkx as nx
# import matplotlib.pyplot as plt
# from sklearn.metrics.pairwise import cosine_similarity
# from sentence_transformers import SentenceTransformer
# from torch_geometric.data import Data
# from torch_geometric.nn import GATConv
# from torch_geometric.utils import from_networkx

# # === 1. Load JSON tags ===
# with open('/content/vitonhd_test_tagged.json') as f:
#     tags_json = json.load(f)

# cloth_dir = "/content/unzipped_drive/clothes_tryon_dataset/test/cloth"
# image_names = sorted(os.listdir(cloth_dir))
# valid_images = [name for name in image_names if name in tags_json]

# print(f"‚úÖ {len(valid_images)} valid images matched with tags.")

# # === 2. Extract text features ===
# model = SentenceTransformer('all-MiniLM-L6-v2')  # You can use mpnet or E5 for better results

# text_features = []
# image_ids = []

# for name in tqdm(valid_images, desc="Encoding tags"):
#     tags = tags_json[name]
#     tag_str = ', '.join([f"{v}: {k}" for k, v in tags['tags'].items() if v])
#     full_desc = f"{tags['category']}: {tag_str}"
#     emb = model.encode(full_desc)
#     text_features.append(emb)
#     image_ids.append(name)

# text_features = torch.tensor(text_features, dtype=torch.float)

# # === 3. Construct similarity graph ===
# similarity_matrix = cosine_similarity(text_features)
# G = nx.Graph()

# threshold = 0.8  # adjust for denser/sparser graph

# for i in range(len(image_ids)):
#     G.add_node(i, x=text_features[i])
#     for j in range(i+1, len(image_ids)):
#         if similarity_matrix[i, j] > threshold:
#             G.add_edge(i, j)

# print(f"‚úÖ Graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges")

# # === 4. Convert to PyG graph ===
# data = from_networkx(G)
# data.x = torch.stack([G.nodes[i]['x'] for i in G.nodes])

# # === 5. Define GAT ===
# class GAT(torch.nn.Module):
#     def __init__(self, in_channels, hidden_channels, out_channels, heads=2):
#         super().__init__()
#         self.gat1 = GATConv(in_channels, hidden_channels, heads=heads)
#         self.gat2 = GATConv(hidden_channels * heads, out_channels, heads=1)

#     def forward(self, data):
#         x, edge_index = data.x, data.edge_index
#         x = F.relu(self.gat1(x, edge_index))
#         x = self.gat2(x, edge_index)
#         return x

# # === 6. Train GAT to learn embeddings ===
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model = GAT(in_channels=text_features.shape[1], hidden_channels=64, out_channels=32).to(device)
# optimizer = torch.optim.Adam(model.parameters(), lr=0.005)
# data = data.to(device)

# for epoch in range(100):
#     model.train()
#     optimizer.zero_grad()
#     out = model(data)
#     loss = -torch.mean(cosine_similarity(out.cpu().detach(), text_features.cpu()))
#     loss.backward()
#     optimizer.step()
#     if epoch % 10 == 0:
#         print(f"Epoch {epoch}: Loss = {loss.item():.4f}")

# # === 7. Visualize Embeddings ===
# from sklearn.decomposition import PCA
# import numpy as np

# emb = model(data).detach().cpu().numpy()
# pca = PCA(n_components=2)
# reduced = pca.fit_transform(emb)

# plt.figure(figsize=(10, 8))
# plt.scatter(reduced[:, 0], reduced[:, 1], c='purple', s=40)
# for i, name in enumerate(image_ids):
#     plt.annotate(name.split('.')[0], (reduced[i, 0], reduced[i, 1]), fontsize=8)
# plt.title("GAT Embeddings (2D PCA)")
# plt.grid(True)
# plt.show()

# Step 1: Extract tag names from JSON
tag_list = []

for item in json_data:
    if "tag_info" in item and item["tag_info"]:
        tags = [tag["tag_name"] for tag in item["tag_info"] if tag.get("tag_name")]
        tag_list.append(tags)

# Sanity check
print(f"üßæ Total items with valid tags: {len(tag_list)}")
print(tag_list[:5])  # Show a sample

# Step 2: Encode using MultiLabelBinarizer
from sklearn.preprocessing import MultiLabelBinarizer

mlb = MultiLabelBinarizer()
tag_vectors = mlb.fit_transform(tag_list)

print(f"‚úÖ Encoded tag vectors shape: {tag_vectors.shape}")
print(f"üß© Unique tag dimensions: {len(mlb.classes_)}")
print(f"üóÇ Sample tags: {mlb.classes_[:10]}")

from sentence_transformers import SentenceTransformer

# Assuming tag_lists is a list of lists like:
# [['item', 'colors', ...], ['item', 'prints', ...], ...]

# Join tags to form sentence-like input
tag_sentences = [' '.join(tags) for tags in tag_list]

# Use a transformer to encode
model = SentenceTransformer('all-MiniLM-L6-v2')
encoded_vectors = model.encode(tag_sentences, convert_to_tensor=True)

import json
import torch
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
from sklearn.neighbors import NearestNeighbors
from torch_geometric.data import Data
from torch_geometric.nn import GATConv
from sentence_transformers import SentenceTransformer

# CONFIGS
json_path = "/content/vitonhd_test_tagged.json"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
embedding_model = SentenceTransformer("all-MiniLM-L6-v2").to(device)

# STEP 1: Load tags from JSON file
with open(json_path, 'r') as f:
    full_json = json.load(f)

data_vectors = []
filenames = []

for item in full_json["data"]:
    tags = item.get("tag_info", [])
    if not tags:
        continue
    tag_text = ", ".join([f"{tag['tag_category']}: {tag['tag_name']}" for tag in tags])
    data_vectors.append(tag_text)
    filenames.append(item["file_name"])

print(f"‚úÖ Loaded {len(data_vectors)} tag sets from JSON")

# STEP 2: Encode with MiniLM
with torch.no_grad():
    encoded_vectors = embedding_model.encode(data_vectors, convert_to_tensor=True).to(device)
print(f"‚úÖ Embedding shape: {encoded_vectors.shape}")

# STEP 3: Build similarity graph
def build_graph(encoded_vectors, k=5):
    nbrs = NearestNeighbors(n_neighbors=k + 1, algorithm='ball_tree').fit(encoded_vectors.cpu())
    distances, indices = nbrs.kneighbors(encoded_vectors.cpu())
    edge_index = []
    for i, neighbors in enumerate(indices):
        for neighbor in neighbors[1:]:
            edge_index.append([i, neighbor])
            edge_index.append([neighbor, i])
    return torch.tensor(edge_index, dtype=torch.long).t().contiguous().to(device)

edge_index = build_graph(encoded_vectors, k=5)
print(f"‚úÖ Graph edges shape: {edge_index.shape}")

# STEP 4: GAT model
class GATEncoder(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.gat1 = GATConv(in_channels, 256, heads=2)
        self.gat2 = GATConv(512, out_channels, heads=1)

    def forward(self, x, edge_index):
        x = F.relu(self.gat1(x, edge_index))
        return self.gat2(x, edge_index)

# STEP 5: Triplet mining
def get_triplets(embeddings, margin=0.2):
    triplets = []
    num_samples = embeddings.shape[0]
    for anchor in range(num_samples):
        dists = torch.norm(embeddings[anchor] - embeddings, dim=1)
        dists[anchor] = float('inf')  # Avoid self as positive
        positive = torch.argmin(dists).item()
        negatives = torch.where(dists > margin)[0]
        if len(negatives) > 0:
            negative = negatives[torch.randint(len(negatives), (1,))].item()
            triplets.append((anchor, positive, negative))
    return triplets

# STEP 6: Training
gat = GATEncoder(in_channels=384, out_channels=128).to(device)
optimizer = torch.optim.Adam(gat.parameters(), lr=1e-3)
triplet_loss_fn = nn.TripletMarginLoss(margin=0.5)

data = Data(x=encoded_vectors, edge_index=edge_index)

for epoch in range(10):
    gat.train()
    optimizer.zero_grad()
    z = gat(data.x, data.edge_index)

    triplets = get_triplets(z)
    if not triplets:
        print(f"‚ö†Ô∏è No valid triplets found in epoch {epoch}")
        continue

    a, p, n = zip(*triplets)
    loss = triplet_loss_fn(z[list(a)], z[list(p)], z[list(n)])
    loss.backward()
    optimizer.step()
    print(f"‚úÖ Epoch {epoch + 1}/10 - Loss: {loss.item():.4f}")

# Final embeddings
gat.eval()
with torch.no_grad():
    final_embeddings = gat(data.x, data.edge_index).cpu()

print("‚úÖ Training complete. Final embeddings ready.")

import os
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image

# Path to cloth images (MUST match filenames in original JSON)
cloth_dir = "/content/unzipped_drive/clothes_tryon_dataset/test/cloth"

# Ensure we have same number of images and embeddings
assert len(filenames) == final_embeddings.shape[0], "Mismatch in filenames and embeddings"

# Normalize embeddings
emb_norm = F.normalize(final_embeddings, dim=1)

# Compute cosine similarity matrix
sim_matrix = emb_norm @ emb_norm.T  # shape: [N, N]

# Show 10 random queries and their top-5 most similar cloths
num_queries = 10
top_k = 5
query_indices = np.random.choice(len(filenames), size=num_queries, replace=False)

fig, axes = plt.subplots(num_queries, top_k + 1, figsize=(15, 3 * num_queries))

for row, query_idx in enumerate(query_indices):
    sims = sim_matrix[query_idx]
    top_indices = torch.topk(sims, k=top_k + 1).indices.tolist()  # includes self

    for col, img_idx in enumerate(top_indices):
        if img_idx == query_idx:
            title = "Query"
        else:
            title = f"Top {col}"

        img_path = os.path.join(cloth_dir, filenames[img_idx])
        img = Image.open(img_path).convert("RGB")N

        axes[row, col].imshow(img)
        axes[row, col].axis('off')
        axes[row, col].set_title(title, fontsize=10)

plt.tight_layout()
plt.show()

import os
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
from sklearn.neighbors import NearestNeighbors
from torch_geometric.data import Data
from torch_geometric.nn import GATConv
from sentence_transformers import SentenceTransformer
import numpy as np

# --- CONFIG ---
json_path = "/content/vitonhd_test_tagged.json"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
embedding_model = SentenceTransformer("all-MiniLM-L6-v2").to(device)
print(f"‚úÖ Using device: {device}")

# --- Load JSON ---
with open(json_path, 'r') as f:
    full_json = json.load(f)

data_vectors, filenames = [], []
for item in full_json["data"]:
    tags = item.get("tag_info", [])
    if not tags: continue
    tag_text = ", ".join([f"{tag['tag_category']}: {tag['tag_name']}" for tag in tags])
    data_vectors.append(tag_text)
    filenames.append(item["file_name"])

print(f"‚úÖ Loaded {len(data_vectors)} tag sets")

# --- Encode Tags ---
with torch.no_grad():
    encoded_vectors = embedding_model.encode(data_vectors, convert_to_tensor=True).to(device)
print(f"‚úÖ Encoded vectors shape: {encoded_vectors.shape}")

# --- Build Similarity Graph ---
def build_graph(encoded_vectors, k=8):
    nbrs = NearestNeighbors(n_neighbors=k + 1).fit(encoded_vectors.cpu())
    _, indices = nbrs.kneighbors(encoded_vectors.cpu())
    edge_index = [[i, j] for i, row in enumerate(indices) for j in row[1:]]
    edge_index += [[j, i] for i, j in edge_index]  # Undirected
    return torch.tensor(edge_index, dtype=torch.long).t().contiguous().to(device)

edge_index = build_graph(encoded_vectors)
print(f"‚úÖ Graph edge_index shape: {edge_index.shape}")

# --- GAT Network ---
class DeepGATEncoder(nn.Module):
    def __init__(self, in_dim=384, hidden_dim=256, out_dim=128, heads=4):
        super().__init__()
        self.gat1 = GATConv(in_dim, hidden_dim, heads=heads, dropout=0.2)
        self.gat2 = GATConv(hidden_dim * heads, hidden_dim, heads=2, dropout=0.2)
        self.gat3 = GATConv(hidden_dim * 2, out_dim, heads=1, dropout=0.1)
        self.norm1 = nn.LayerNorm(hidden_dim * heads)
        self.norm2 = nn.LayerNorm(hidden_dim * 2)

    def forward(self, x, edge_index):
        x = F.relu(self.norm1(self.gat1(x, edge_index)))
        x = F.relu(self.norm2(self.gat2(x, edge_index)))
        return self.gat3(x, edge_index)

# --- Triplet Mining (semi-hard) ---
def get_triplets(embeddings, margin=0.5):
    dists = torch.cdist(embeddings, embeddings, p=2)
    triplets = []
    for anchor in range(len(embeddings)):
        device = embeddings.device
        eye = torch.eye(len(embeddings), device=device)
        pos = torch.argmin(dists[anchor] + eye[anchor] * 1e6)
        neg_mask = dists[anchor] > (dists[anchor][pos] + margin)
        neg_candidates = torch.where(neg_mask)[0]
        if len(neg_candidates):
            neg = neg_candidates[torch.randint(len(neg_candidates), (1,))].item()
            triplets.append((anchor, pos, neg))
    return triplets

# --- Training ---
model = DeepGATEncoder().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)
loss_fn = nn.TripletMarginLoss(margin=0.5)
data = Data(x=encoded_vectors, edge_index=edge_index)

best_loss = float('inf')
patience = 3
no_improve = 0

for epoch in range(20):
    model.train()
    optimizer.zero_grad()
    z = model(data.x, data.edge_index)

    triplets = get_triplets(z)
    if not triplets:
        print(f"‚ö†Ô∏è No valid triplets found")
        continue

    a, p, n = zip(*triplets)
    loss = loss_fn(z[list(a)], z[list(p)], z[list(n)])
    loss.backward()
    optimizer.step()

    print(f"‚úÖ Epoch {epoch+1} - Loss: {loss.item():.4f}")

    # Early stopping
    if loss.item() < best_loss:
        best_loss = loss.item()
        no_improve = 0
        best_model_state = model.state_dict()
    else:
        no_improve += 1
        if no_improve >= patience:
            print("‚èπÔ∏è Early stopping triggered")
            break

# --- Save final embeddings ---
model.load_state_dict(best_model_state)
model.eval()
with torch.no_grad():
    final_embeddings = model(data.x, data.edge_index).cpu()

print("‚úÖ Training complete. Embeddings ready.")